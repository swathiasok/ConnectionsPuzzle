{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b83c516-4939-4755-8b5c-7dee8674f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb61c53-65b2-4182-8897-7afc71223593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install numpy==1.26.4\n",
    "# !{sys.executable} -m pip install unsloth vllm==0.7.2\n",
    "# !{sys.executable} -m pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72475ee0-fb8f-4b8b-9dd2-b470033ee240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5f6133-efb3-43a7-95ed-bc13e0436e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-21 21:59:43 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2715ed-40aa-476a-a0cd-28f9ec089ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df57a8aa-f048-4e70-9335-631d7fa23887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connections_questions(SYSTEM_PROMPT, split=\"train\"):\n",
    "    data = load_dataset('csv', data_files='dataset/final_transformed_connections.csv')[split]\n",
    "    data = data.train_test_split(test_size=0.1, seed=3407)\n",
    "    train_data = data['train']\n",
    "    test_data  = data['test']\n",
    "    # each x has 'questions' (the 16‚Äêword list) and 'answer' (the reference)\n",
    "    train_data = train_data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'assistant', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user',      'content': x['questions']}\n",
    "        ],\n",
    "        'answer': x['answer']\n",
    "    })\n",
    "    test_data = test_data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'assistant', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user',      'content': x['questions']}\n",
    "        ],\n",
    "        'answer': x['answer']\n",
    "    })\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8cb197-4ea9-4f3c-8a30-d78b41e63d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_SYSTEM_PROMPT = '''\n",
    "You are playing the NY Times Connections game. Your task is to categorize 16 given words into exactly 4 groups of 4 words each, based on shared common themes.\n",
    "\n",
    "Only output your final solution in the following format:\n",
    "<answer>\n",
    "[['WORD1', 'WORD2', 'WORD3', 'WORD4'],\n",
    " ['WORD5', 'WORD6', 'WORD7', 'WORD8'],\n",
    " ['WORD9', 'WORD10', 'WORD11', 'WORD12'],\n",
    " ['WORD13', 'WORD14', 'WORD15', 'WORD16']]\n",
    "</answer>\n",
    "\n",
    "Rules:\n",
    "\t‚Ä¢ Each word must belong to one group only.\n",
    "\t‚Ä¢ Groups must have a clear, shared theme (e.g., weather, NBA teams, keyboard keys, etc.).\n",
    "\t‚Ä¢ Do not include any words not present in the input list.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "USER: [BUCKS, HAIL, JAZZ, SHIFT, LEVEL, MOM, SNOW, RACECAR, SLEET, TAB, KAYAK, RETURN, OPTION, NETS, RAIN, HEAT]\n",
    "\n",
    "SOLUTION:\n",
    "[['HAIL', 'RAIN', 'SLEET', 'SNOW'],\n",
    " ['BUCKS', 'HEAT', 'JAZZ', 'NETS'],\n",
    " ['OPTION', 'RETURN', 'SHIFT', 'TAB'],\n",
    " ['KAYAK', 'LEVEL', 'MOM', 'RACECAR']]\n",
    "\n",
    "Explanation:\n",
    "- WEATHER TERMS: 'HAIL', 'RAIN', 'SLEET', 'SNOW'\n",
    "- NBA TEAMS: 'BUCKS', 'HEAT', 'JAZZ', 'NETS'\n",
    "- KEYBOARD KEYS: 'OPTION', 'RETURN', 'SHIFT', 'TAB'\n",
    "- PALINDROMES: 'KAYAK', 'LEVEL', 'MOM', 'RACECAR'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd85b56-8159-4089-b04f-a46345fd216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds, test_ds = get_connections_questions(SFT_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cc7c523-6430-4d39-95f7-0144370faf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRPO_SYSTEM_PROMPT = '''\n",
    "You are playing the NY Times Connections game. Your task is to categorize 16 given words into exactly 4 groups of 4 words each, based on shared common themes.\n",
    "\n",
    "Solve the puzzle using these clear steps:\n",
    "\n",
    "1. THINK STEP-BY-STEP: Begin by carefully analyzing the words within <think> tags. Identify their meanings, relationships, and possible groupings logically.\n",
    "\n",
    "2. PROVIDE FINAL ANSWER: After clearly grouping and justifying all four sets, provide ONLY your final solution within <answer> tags. Format your solution exactly as shown below.\n",
    "\n",
    "Example:\n",
    "<think>\n",
    "......\n",
    "......\n",
    "</think>\n",
    "<answer>\n",
    "[['HAIL', 'RAIN', 'SLEET', 'SNOW'],\n",
    " ['BUCKS', 'HEAT', 'JAZZ', 'NETS'],\n",
    " ['OPTION', 'RETURN', 'SHIFT', 'TAB'],\n",
    " ['KAYAK', 'LEVEL', 'MOM', 'RACECAR']]\n",
    "</answer>\n",
    "\n",
    "Important Notes:\n",
    "- Categories should be specific\n",
    "- Words cannot appear in more than one group.\n",
    "- Categories can include compound words, shared prefixes/suffixes, pop culture references, or common phrases.\n",
    "- DO NOT ADD NEW WORDS THAT ARE NOT MENTIONED IN THE QUESTION. USE ONLY WORDS MENTIONED AND GROUP THEM\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "USER: [BUCKS, HAIL, JAZZ, SHIFT, LEVEL, MOM, SNOW, RACECAR, SLEET, TAB, KAYAK, RETURN, OPTION, NETS, RAIN, HEAT]\n",
    "\n",
    "SOLUTION:\n",
    "[['HAIL', 'RAIN', 'SLEET', 'SNOW'],\n",
    " ['BUCKS', 'HEAT', 'JAZZ', 'NETS'],\n",
    " ['OPTION', 'RETURN', 'SHIFT', 'TAB'],\n",
    " ['KAYAK', 'LEVEL', 'MOM', 'RACECAR']]\n",
    "\n",
    "Explanation:\n",
    "- WEATHER TERMS: 'HAIL', 'RAIN', 'SLEET', 'SNOW'\n",
    "- NBA TEAMS: 'BUCKS', 'HEAT', 'JAZZ', 'NETS'\n",
    "- KEYBOARD KEYS: 'OPTION', 'RETURN', 'SHIFT', 'TAB'\n",
    "- PALINDROMES: 'KAYAK', 'LEVEL', 'MOM', 'RACECAR'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e61279-7e01-4e86-ac45-bbf080d0cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_connections_questions(GRPO_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75fccaee-fdcc-4890-92f0-8b0878c38dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# 2) Initialize model + LoRA adapter\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "\n",
    "# max_seq_length = 1024\n",
    "# lora_rank       = 8\n",
    "\n",
    "# # load base model\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name            = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "#     max_seq_length        = max_seq_length,\n",
    "#     load_in_4bit          = False,    \n",
    "#     fast_inference        = True,\n",
    "#     max_lora_rank         = lora_rank\n",
    "# )\n",
    "\n",
    "# # wrap with PEFT\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model              = model,\n",
    "#     r                  = lora_rank,\n",
    "#     target_modules     = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "#     lora_alpha         = lora_rank,\n",
    "#     random_state       = 3407,\n",
    "# )\n",
    "\n",
    "# # Make sure we also have a HF‚Äêstyle tokenizer\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\", use_fast=True)\n",
    "# hf_tokenizer.pad_token_id = hf_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "621fd05f-1cc1-4f87-a3e7-d9da41fd1e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# # 3) Preprocess: pad labels to 1024\n",
    "# # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# def preprocess_sft(batch):\n",
    "#     inputs = [\n",
    "#         tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=False)\n",
    "#         for p in batch[\"prompt\"]\n",
    "#     ]\n",
    "#     model_inputs = hf_tokenizer(\n",
    "#         inputs,\n",
    "#         max_length = max_seq_length,\n",
    "#         padding    = \"max_length\",\n",
    "#         truncation = True,\n",
    "#     )\n",
    "#     labels = hf_tokenizer(\n",
    "#         batch[\"answer\"],\n",
    "#         max_length = max_seq_length,    # ‚Üê pad answers out to 1024\n",
    "#         padding    = \"max_length\",\n",
    "#         truncation = True,\n",
    "#     )\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "# train_enc = train_ds.map(\n",
    "#     preprocess_sft, batched=True, remove_columns=train_ds.column_names\n",
    "# )\n",
    "# test_enc  = test_ds.map(\n",
    "#     preprocess_sft, batched=True, remove_columns=test_ds.column_names\n",
    "# )\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(\n",
    "#     tokenizer           = hf_tokenizer,\n",
    "#     model               = model.model,   # unwrapped base\n",
    "#     label_pad_token_id  = -100,          # ignore padded labels\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3af90ff-5714-4757-9645-731bb0007119",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# # 4) Seq2SeqTrainer with padded labels\n",
    "# # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# sft_args = TrainingArguments(\n",
    "#     output_dir                  = \"sft_outputs\",\n",
    "#     per_device_train_batch_size = 1,\n",
    "#     gradient_accumulation_steps = 16,\n",
    "#     num_train_epochs            = 3,\n",
    "#     learning_rate               = 5e-5,\n",
    "#     fp16                        = not is_bfloat16_supported(),\n",
    "#     bf16                        = is_bfloat16_supported(),\n",
    "#     logging_steps               = 10,\n",
    "#     save_steps                  = 100,\n",
    "#     save_total_limit            = 2,\n",
    "# )\n",
    "\n",
    "# sft_trainer = Trainer(\n",
    "#     model         = model,\n",
    "#     args          = sft_args,\n",
    "#     train_dataset = train_enc,\n",
    "#     eval_dataset  = test_enc,\n",
    "#     tokenizer     = hf_tokenizer,\n",
    "#     data_collator = DataCollatorForSeq2Seq(\n",
    "#         tokenizer          = hf_tokenizer,\n",
    "#         model              = model.model,\n",
    "#         label_pad_token_id = -100,\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2840c624-79e9-4b1a-9c66-b9e454c1d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the SFT pass\n",
    "# sft_trainer.train()\n",
    "# model.save_lora(\"sft_saved_lora_3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eedd49be-cb51-405c-9dff-d76437b0e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "lora_rank = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f80f4f0a-aa75-4583-9a50-b45bcc9361b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0. vLLM: 0.7.2.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 2. Max memory: 44.451 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 89.39%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 44.45 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 33.73 GB. Also swap space = 6 GB.\n",
      "INFO 04-21 22:01:39 config.py:542] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-21 22:01:39 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":320}, use_cached_outputs=False, \n",
      "INFO 04-21 22:01:39 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 04-21 22:01:40 model_runner.py:1110] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W421 22:01:40.016519132 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 22:01:40 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2141cc3dd16443d68c73e327d1f91785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 22:01:41 model_runner.py:1115] Loading model weights took 6.0316 GB\n",
      "INFO 04-21 22:01:41 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-21 22:01:43 worker.py:267] Memory profiling takes 1.13 seconds\n",
      "INFO 04-21 22:01:43 worker.py:267] the current vLLM instance can use total_gpu_memory (44.45GiB) x gpu_memory_utilization (0.89) = 39.74GiB\n",
      "INFO 04-21 22:01:43 worker.py:267] model weights take 6.03GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 32.16GiB.\n",
      "INFO 04-21 22:01:43 executor_base.py:110] # CUDA blocks: 18819, # CPU blocks: 3510\n",
      "INFO 04-21 22:01:43 executor_base.py:115] Maximum concurrency for 1024 tokens per request: 294.05x\n",
      "INFO 04-21 22:01:46 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:26<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 22:02:12 model_runner.py:1562] Graph capturing finished in 26 secs, took 0.39 GiB\n",
      "INFO 04-21 22:02:12 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 30.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoRARequest(lora_name='0', lora_int_id=0, lora_path='sft_saved_lora_3B', lora_tensors=None, lora_config=(None,), lora_local_path=None, long_lora_max_len=None, base_model_name=None, lora_embeddings=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# 4) GRPO stage, initialized from SFT adapter\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "\n",
    "# re‚Äëload the same base & adapter structure\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name             = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length         = max_seq_length,\n",
    "    load_in_4bit           = False,\n",
    "    fast_inference         = True,\n",
    "    max_lora_rank          = lora_rank,\n",
    "    gpu_memory_utilization = 0.9,\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model          = model,\n",
    "    r              = lora_rank,\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha     = lora_rank,\n",
    ")\n",
    "# load your SFT‚Äêtrained adapter\n",
    "model.load_lora(\"sft_saved_lora_3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f76aa8c1-1448-4cb5-9e5e-49f8be3ec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thinking_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function for including thinking tags\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        try:\n",
    "            reward = 0.0\n",
    "            # Extract all thinking blocks\n",
    "            for message in completion:\n",
    "                if message[\"role\"] == \"assistant\" and message.get(\"content\"):\n",
    "                    content = message[\"content\"]\n",
    "\n",
    "                    # Count opening and closing tags\n",
    "                    opening_tags = len(re.findall(r\"<think>\", content))\n",
    "                    closing_tags = len(re.findall(r\"</think>\", content))\n",
    "\n",
    "                    if opening_tags == 0 or closing_tags == 0:\n",
    "                        continue\n",
    "\n",
    "                    if opening_tags == closing_tags:\n",
    "                        reward += 0.5\n",
    "                    else:\n",
    "                        reward += 0.1\n",
    "            reward = min(reward, 1.5)\n",
    "            rewards.append(reward)\n",
    "        except Exception as e:\n",
    "            print(f\"{RED}Error in thinking_reward_func: {e}{RESET}\")\n",
    "            rewards.append(0.0)\n",
    "    assert len(rewards) == len(completions)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def answer_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function for including answer tags\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        try:\n",
    "            reward = 0.0\n",
    "            # Extract all answer blocks\n",
    "            for message in completion:\n",
    "                if message[\"role\"] == \"assistant\" and message.get(\"content\"):\n",
    "                    content = message[\"content\"]\n",
    "\n",
    "                    # Count opening and closing tags\n",
    "                    opening_tags = len(re.findall(r\"<answer>\", content))\n",
    "                    closing_tags = len(re.findall(r\"</answer>\", content))\n",
    "\n",
    "                    if opening_tags == 0 or closing_tags == 0:\n",
    "                        continue\n",
    "\n",
    "                    if opening_tags == 1 and closing_tags == 1:\n",
    "                        reward += 0.5\n",
    "                    else:\n",
    "                        reward += 0.1\n",
    "            reward = min(reward, 1.5)\n",
    "            rewards.append(reward)\n",
    "        except Exception as e:\n",
    "            print(f\"{RED}Error in thinking_reward_func: {e}{RESET}\")\n",
    "            rewards.append(0.0)\n",
    "    assert len(rewards) == len(completions)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def correctness_reward_func(completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Safe, robust reward for NYT Connections.\"\"\"\n",
    "    rewards = []\n",
    "    for completion, expected_answer in zip(completions, answer):\n",
    "        # default\n",
    "        reward = 0.0\n",
    "        # 1) Extract last <answer>‚Ä¶</answer> block\n",
    "        predicted_block = None\n",
    "        for msg in completion:\n",
    "# Ensure msg is a dictionary and check its role and content\n",
    "            if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and \"content\" in msg:\n",
    "                matches = re.findall(r\"<answer>(.*?)</answer>\", msg[\"content\"], re.DOTALL)\n",
    "                if matches:\n",
    "                    predicted_block = matches[-1].strip()\n",
    "        \n",
    "        if not predicted_block:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # 2) Safe parse\n",
    "        try:\n",
    "            pred_groups = ast.literal_eval(predicted_block)\n",
    "            exp_groups  = ast.literal_eval(expected_answer)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # 3) Normalize: keep only lists of strings\n",
    "        def clean(groups):\n",
    "            cleaned = []\n",
    "            for g in groups:\n",
    "                if isinstance(g, list):\n",
    "                    cleaned.append([w for w in g if isinstance(w, str)])\n",
    "            return cleaned\n",
    "\n",
    "        pred = clean(pred_groups)\n",
    "        exp  = clean(exp_groups)\n",
    "\n",
    "        # 4) Scoring\n",
    "        # Perfect match\n",
    "        if pred == exp:\n",
    "            rewards.append(6.0)\n",
    "            continue\n",
    "\n",
    "        # +1.5 for each fully correct group (set‚Äêequality)\n",
    "        used = set()\n",
    "        for pg in pred:\n",
    "            for i, eg in enumerate(exp):\n",
    "                if i in used: \n",
    "                    continue\n",
    "                if set(pg) == set(eg):\n",
    "                    reward += 1.5\n",
    "                    used.add(i)\n",
    "                    break\n",
    "\n",
    "        # +0.75 for any 3‚Äêword overlap in unmatched groups\n",
    "        for pg in pred:\n",
    "            for i, eg in enumerate(exp):\n",
    "                if i in used:\n",
    "                    continue\n",
    "                overlap = len([w for w in pg if w in eg])\n",
    "                if overlap == 3:\n",
    "                    reward += 0.75\n",
    "                    used.add(i)\n",
    "                    break\n",
    "\n",
    "        # +0.25 if every predicted group has exactly 4 words\n",
    "        if all(len(pg) == 4 for pg in pred):\n",
    "            reward += 0.25\n",
    "\n",
    "        # +0.5 if no word is repeated across all predicted groups\n",
    "        flat = [w for pg in pred for w in pg]\n",
    "        if len(flat) == len(set(flat)):\n",
    "            reward += 0.5\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    assert len(rewards) == len(completions)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a6b6631-04fe-4dc8-a6a3-a1013b7034de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from rich import print\n",
    "rl_args = GRPOConfig(\n",
    "    use_vllm                     = True,\n",
    "    learning_rate                = 5e-6,\n",
    "    adam_beta1                   = 0.9,\n",
    "    adam_beta2                   = 0.99,\n",
    "    weight_decay                 = 0.1,\n",
    "    warmup_ratio                 = 0.1,\n",
    "    lr_scheduler_type            = \"cosine\",\n",
    "    optim                        = \"adamw_8bit\",\n",
    "    logging_steps                = 1,\n",
    "    bf16                         = is_bfloat16_supported(),\n",
    "    fp16                         = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size  = 1,\n",
    "    gradient_accumulation_steps  = 16,\n",
    "    num_generations              = 4,\n",
    "    max_prompt_length            = 700,\n",
    "    max_completion_length        = 700,\n",
    "    # num_train_epochs             = 1,\n",
    "    max_steps                    = 200,\n",
    "    save_steps                   = 50,\n",
    "    max_grad_norm                = 0.1,\n",
    "    report_to                    = \"none\",\n",
    "    output_dir                   = \"outputs_grpo\",\n",
    "    log_completions              = 5, \n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model            = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs     = [\n",
    "        thinking_reward_func,\n",
    "        correctness_reward_func,\n",
    "        answer_format_reward_func,\n",
    "    ],\n",
    "    args             = rl_args,\n",
    "    train_dataset    = train_ds,  # or a mix of train & test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2098cd5f-74df-4ebb-af3c-aba0ac9a38b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 580 | Num Epochs = 12 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 16 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)\n",
      "Unsloth: Input IDs of length 1029 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1027 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Input IDs of length 1030 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1025 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1036 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1026 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1028 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/200 15:39 < 8:21:01, 0.01 it/s, Epoch 0.39/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / thinking_reward_func</th>\n",
       "      <th>rewards / correctness_reward_func</th>\n",
       "      <th>rewards / answer_format_reward_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.521094</td>\n",
       "      <td>1.157509</td>\n",
       "      <td>303.054688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340625</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.328906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.539844</td>\n",
       "      <td>1.156473</td>\n",
       "      <td>272.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344531</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.332031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.269531</td>\n",
       "      <td>1.039544</td>\n",
       "      <td>280.367188</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.335156</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.293750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.521094</td>\n",
       "      <td>1.087884</td>\n",
       "      <td>273.335938</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.372656</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.335938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.500781</td>\n",
       "      <td>1.228315</td>\n",
       "      <td>275.289062</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.332812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.322656</td>\n",
       "      <td>1.135398</td>\n",
       "      <td>290.351562</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.328906</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.294531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Input IDs of length 1032 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1069 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1031 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1034 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1050 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1035 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1033 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1043 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1037 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "Unsloth: Input IDs of length 1039 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:314\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:25\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch1/vedanth/unsloth_compiled_cache/UnslothGRPOTrainer.py:952\u001b[39m, in \u001b[36m_UnslothGRPOTrainer._prepare_inputs\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    950\u001b[39m all_prompts_text = gather_object(prompts_text)\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.is_main_process:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_prompts_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrpo_trainer_lora_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m     completion_ids = [out.token_ids \u001b[38;5;28;01mfor\u001b[39;00m completions \u001b[38;5;129;01min\u001b[39;00m outputs \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m completions.outputs]\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/utils.py:1086\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1079\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1081\u001b[39m         warnings.warn(\n\u001b[32m   1082\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1083\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1084\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/entrypoints/llm.py:469\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    459\u001b[39m     sampling_params = \u001b[38;5;28mself\u001b[39m.get_default_sampling_params()\n\u001b[32m    461\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    462\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    463\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    466\u001b[39m     guided_options=guided_options_request,\n\u001b[32m    467\u001b[39m     priority=priority)\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/entrypoints/llm.py:1390\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1388\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1391\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1392\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py:1386\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[32m   1383\u001b[39m     execute_model_req.async_callback = \u001b[38;5;28mself\u001b[39m.async_callbacks[\n\u001b[32m   1384\u001b[39m         virtual_engine]\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[32m   1390\u001b[39m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scheduler_config.is_multi_step:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py:138\u001b[39m, in \u001b[36mExecutorBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_model\u001b[39m(\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[32m    137\u001b[39m ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:51\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     50\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/utils.py:2220\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2219\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/worker/worker_base.py:413\u001b[39m, in \u001b[36mLocalOrDistributedWorkerBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    409\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_execute_time):\n\u001b[32m    410\u001b[39m         orig_model_execute_time = intermediate_tensors.tensors.get(\n\u001b[32m    411\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel_execute_time\u001b[39m\u001b[33m\"\u001b[39m, torch.tensor(\u001b[32m0\u001b[39m)).item()\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m model_execute_time = time.perf_counter() - start_time\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    424\u001b[39m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py:1775\u001b[39m, in \u001b[36mModelRunner.execute_model\u001b[39m\u001b[34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[39m\n\u001b[32m   1772\u001b[39m     model_input.async_callback()\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m output: SamplerOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1780\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_forward_time\n\u001b[32m   1781\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1782\u001b[39m     model_forward_end.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:557\u001b[39m, in \u001b[36mLlamaForCausalLM.sample\u001b[39m\u001b[34m(self, logits, sampling_metadata)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits: torch.Tensor,\n\u001b[32m    556\u001b[39m            sampling_metadata: SamplingMetadata) -> Optional[SamplerOutput]:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     next_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py:288\u001b[39m, in \u001b[36mSampler.forward\u001b[39m\u001b[34m(self, logits, sampling_metadata)\u001b[39m\n\u001b[32m    285\u001b[39m logprobs = torch.log_softmax(logits, dim=-\u001b[32m1\u001b[39m, dtype=torch.float)\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m maybe_deferred_sample_results, maybe_sampled_tokens_tensor = \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_gpu_probs_tensor:\n\u001b[32m    298\u001b[39m     \u001b[38;5;66;03m# Since we will defer sampler result Pythonization,\u001b[39;00m\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# preserve GPU-side tensors in support of later\u001b[39;00m\n\u001b[32m    300\u001b[39m     \u001b[38;5;66;03m# deferred pythonization of logprobs\u001b[39;00m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py:853\u001b[39m, in \u001b[36m_sample\u001b[39m\u001b[34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[39m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sample\u001b[39m(\n\u001b[32m    834\u001b[39m     probs: torch.Tensor,\n\u001b[32m    835\u001b[39m     logprobs: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    839\u001b[39m     modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    840\u001b[39m ) -> SampleReturnType:\n\u001b[32m    841\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[33;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    851\u001b[39m \u001b[33;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[32m    852\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py:822\u001b[39m, in \u001b[36m_sample_with_torch\u001b[39m\u001b[34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[39m\n\u001b[32m    810\u001b[39m maybe_deferred_args = SampleResultArgsType(\n\u001b[32m    811\u001b[39m     sampling_metadata=sampling_metadata,\n\u001b[32m    812\u001b[39m     sample_metadata=sample_metadata,\n\u001b[32m   (...)\u001b[39m\u001b[32m    815\u001b[39m     beam_search_logprobs=beam_search_logprobs,\n\u001b[32m    816\u001b[39m     sample_results_dict=sample_results_dict)\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampling_metadata.skip_sampler_cpu_output:\n\u001b[32m    819\u001b[39m     \u001b[38;5;66;03m# GPU<->CPU sync happens here.\u001b[39;00m\n\u001b[32m    820\u001b[39m     \u001b[38;5;66;03m# This also converts the sampler output to a Python object.\u001b[39;00m\n\u001b[32m    821\u001b[39m     \u001b[38;5;66;03m# Return Pythonized sampler result & sampled token ids\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_pythonized_sample_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaybe_deferred_args\u001b[49m\u001b[43m)\u001b[49m, sampled_token_ids_tensor\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    825\u001b[39m     \u001b[38;5;66;03m# Defer sampler result Pythonization; return deferred\u001b[39;00m\n\u001b[32m    826\u001b[39m     \u001b[38;5;66;03m# Pythonization args & sampled token ids\u001b[39;00m\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    828\u001b[39m         maybe_deferred_args,\n\u001b[32m    829\u001b[39m         sampled_token_ids_tensor,\n\u001b[32m    830\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py:687\u001b[39m, in \u001b[36mget_pythonized_sample_results\u001b[39m\u001b[34m(sample_result_args)\u001b[39m\n\u001b[32m    685\u001b[39m     sample_results = _greedy_sample(seq_groups, greedy_samples)\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType.RANDOM, SamplingType.RANDOM_SEED):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     sample_results = \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sampling_type == SamplingType.BEAM:\n\u001b[32m    690\u001b[39m     sample_results = _beam_search_sample(seq_groups,\n\u001b[32m    691\u001b[39m                                          beam_search_logprobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py:486\u001b[39m, in \u001b[36m_random_sample\u001b[39m\u001b[34m(selected_seq_groups, random_samples)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run random sampling on a given samples.\u001b[39;00m\n\u001b[32m    474\u001b[39m \n\u001b[32m    475\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    483\u001b[39m \u001b[33;03m    seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# Find the maximum n value of the prompt phase requests.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m random_samples = \u001b[43mrandom_samples\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m sample_idx = \u001b[32m0\u001b[39m\n\u001b[32m    488\u001b[39m results: SampleResultType = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8adfb79-0f6c-461b-9da8-16e01ac5a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora_3B_comp1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57c17230-211b-48ed-9cf8-0f5a46cfb3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.82s/it, est. speed input: 81.56 toks/s, output: 68.79 toks/s]\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : GRPO_SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"[FLY, SINK, SHOWER, SALSA, TAP, RACE, DIP, MODERN, FALL, SWING, CARROT, TEAR, DROP, TALK, BLAZE, BOOM]\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    # lora_request = model.load_lora(\"grpo_saved_lora_3B\"),\n",
    ")[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "373dbe3e-f735-4462-8dbc-40d60d68c21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\n- FLY, SHOWER, SHOOTER, DIP all relate to SHOOT, implying they are either actions of shooting or types of shooting, or associated with shooting like a sport or device.\\n- SINK, RACE, SWING, DROP all can imply a loss or failure in these words; however, SINK and SWING also relate to furniture, so considering different aspects of these words may also be valid. Also, the words TALK, TEAR, and BLAZE also imply loss or failure in the context of public speaking or burning down something. MODERN and CARROT do not fit into these categories. \\n- FLY, SHOOT, RACE, and DIP might be associated with performance and speed in sports, while SINK, SWING, and DROP relate to falls, SINK to furniture, and TEAR to tears and could also be associated with loss of control in sports, TALK and BLAZE to loss of control or mess in public, and MODERN and CARROT do not fit into these categories.\\n- FLY, SHOWER, SINK, TALK all have multiple related meanings, TALK also implies loss of control in public as in shouting or yelling.\\n- RACE, SHOOTER, SWING all imply performance and speed, or skill, and SHOWER and TALK could be related to a show or speech.\\n- DIP, DROP, SHOOT, SINK all imply loss or a fall.\\n- BLAZE, TEAR, TALK, SHOWER all relate to loss of control, SHOWER, SINK, TALK, and SHOOT all could be associated with a performance, TALK also implies public loss of control.\\n\\nConsidering different possible interpretations, SHOOTER, SINK, SHOWER, TALK, DIP, DROP, RACE, TEAR, and BLAZE seem like the most direct connections, with SHOOTER implying shooting and SHOWER and SHOOT also implying a performance. Considering multiple possible meanings and interpretations, these words seem the most logically connected.\\n\\n</think>\\n<answer>\\n[['FLY', 'SHOWER', 'SHOOTER', 'SINK'],\\n ['RACE', 'SHOOT', 'SWING', 'DIP'],\\n ['TALK', 'TEAR', '\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866cff8-f0d6-42d2-b9c7-47149b688835",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['DIP', 'DROP', 'FALL', 'SINK'], ['BLAZE', 'FLY', 'RACE', 'TEAR'], ['MODERN', 'SALSA', 'SWING', 'TAP'], ['BOOM', 'CARROT', 'SHOWER', 'TALK']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vedskernel1",
   "language": "python",
   "name": "vedskernel1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
